{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import os, argparse\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import librosa\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import decimate, butter, lfilter\n",
    "import IPython.display as ipd\n",
    "from scipy import signal\n",
    "from ops import silence_filtering, upsample\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "import re\n",
    "from skimage.transform import resize\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 8192 #--dimension -> dimension of patches --use -1 for no patching\n",
    "sr = 16000 #args.sr -> audio sampling rate\n",
    "scale = 4 #args.scale -> scaling factor\n",
    "low_pass = True #args.low_pass -> apply low-pass filter when generating low-res patches\n",
    "stride = 2048  #args.stride -> 8192*0.75 = 2048 (Time Frequency Networks For Audio Super-Resolu)\n",
    "batch_size = 128 # sia tfnet che kuleshov usano 128\n",
    "trim_silence = False\n",
    "silence_trash = 0 #DA DEFINIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence_filtering(sig, top_db):\n",
    "    #deve restituire solo il segnale filtrato\n",
    "    filt_sig, _ = librosa.effects.trim(sig, top_db=trim_silence,  frame_length=2048, hop_length=512)\n",
    "    return filt_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath('.'))\n",
    "output_dir = \"\\\\processedData\\\\speaker1\\\\train&validation\\\\\"\n",
    "out = ROOT_DIR + output_dir #args.out -> path to output h5 archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_list = [id_ for id_ in os.listdir(ROOT_DIR + data_path)] #id_list conta 109 speacker\n",
    "# first_audio = ROOT_DIR + data_path + id_list[0] + '\\\\p225_366.wav'\n",
    "# sig, rate = librosa.load(first_audio, sr=sr, mono=False)\n",
    "# plt.plot(sig)\n",
    "# ipd.Audio(sig, rate= rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(how = 'training'):\n",
    "    \n",
    "    if how == 'training':\n",
    "        tr = True\n",
    "    elif how == 'validation':\n",
    "        tr = False\n",
    "    else:\n",
    "        raise ValueError('you should choose between training or validation')\n",
    "        return None #necessario?\n",
    "    \n",
    "    data_dir = r'C:\\Users\\Giobi\\Tesi\\rawData\\VCTK-Corpus\\wav48\\p225'\n",
    "    in_dir = r'C:\\Users\\Giobi\\Tesi\\processedData\\speaker1'\n",
    "    if tr:\n",
    "        inputfiles = in_dir + '\\speaker1-train-files.txt' \n",
    "    else:\n",
    "        inputfiles = in_dir + '\\speaker1-val-files.txt' \n",
    "    file_list = []\n",
    "    ID_list = []\n",
    "    file_extensions = set(['.wav'])\n",
    "    save_examples=False\n",
    "    with open(inputfiles) as f:\n",
    "        for line in f:\n",
    "            filename = line.strip()\n",
    "            ext = os.path.splitext(filename)[1]\n",
    "            if ext in file_extensions:\n",
    "                file_list.append(os.path.join(data_dir, filename))\n",
    "    num_files = len(file_list)\n",
    "    # patches to extract and their size\n",
    "    d, d_lr = dimension, dimension\n",
    "    s, s_lr = stride, stride\n",
    "    hr_patches, lr_patches = list(), list()\n",
    "\n",
    "    for j, file_path in enumerate(file_list):\n",
    "        ID = int(re.search('p\\d\\d\\d', file_path).group(0)[1:]) #originariamente era int(re.search('p\\d\\d\\d/', file_path).group(0)[1:-1])\n",
    "        # L'impostazione originale aveva due problemi. La regex ritornava un NoneType object, perciò è stato rimosso il backslash finale. \n",
    "        # Inoltre per ricavare l'ID dello speacker (es. p255 -> ID = 255), originariamente si aveva .group(0)[1:-1]. Tuttavia in quel caso veniva ID = 22 e non 225    \n",
    "\n",
    "        # load audio file\n",
    "        x, fs = librosa.load(file_path, sr=sr)\n",
    "        \n",
    "        if ((how == 'training')&(trim_silence == True)):\n",
    "            x = silence_filtering(x)\n",
    "        \n",
    "        # crop so that it works with scaling ratio\n",
    "        x_len = len(x)\n",
    "        x = x[ : x_len - (x_len % scale)] #sostanzialmente questa operazione permette di ottenere una lunghezza di x (numero di campioni) adeguata allo scaling ratio.\n",
    "        # Es: scale = 2 -> se il numero di campioni (lunghezza di x) è pari, allora non succede nulla. Se è dispari, invece, l'ultimo campione viene rimosso. \n",
    "\n",
    "        # generate low-res version\n",
    "        if low_pass:\n",
    "            x_lr = decimate(x, scale)\n",
    "        else:\n",
    "            x_lr = np.array(x[0::scale]) # la lunghezza è pari a x/scale (approssimazione per eccesso). \n",
    "                                         # Sostanzialmente in questo modo si prendono campioni a salti. \n",
    "                                         # Es. a = np.arange(5)\n",
    "                                         #     a[0::2] -> array([0, 2, 4])\n",
    "        x_lr = upsample(x_lr, scale) #interpolate low-res patches with cubic splines. \n",
    "                                     #After this line of code len(x_lr) is equal to len(x)\n",
    "        assert len(x) % scale == 0\n",
    "        assert len(x_lr) == len(x)\n",
    "        assert x.dtype == np.float32\n",
    "        assert x_lr.dtype == np.float32\n",
    "\n",
    "        # generate patches\n",
    "        max_i = len(x) - d + 1\n",
    "        for i in range(0, max_i, s):\n",
    "            i_lr = i\n",
    "            hr_patch = np.array( x[i : i+d] )\n",
    "            lr_patch = np.array( x_lr[i_lr : i_lr+d_lr] )\n",
    "\n",
    "            assert len(hr_patch) == d\n",
    "            assert len(lr_patch) == d_lr\n",
    "\n",
    "            hr_patches.append(hr_patch.reshape((d,1)))\n",
    "            lr_patches.append(lr_patch.reshape((d_lr,1)))\n",
    "            ID_list.append(ID)\n",
    "    \n",
    "    # crop # of patches so that it's a multiple of mini-batch size\n",
    "    num_patches = len(hr_patches)\n",
    "    num_to_keep = int(np.floor(num_patches / batch_size) * batch_size)\n",
    "    hr_patches = np.array(hr_patches[:num_to_keep])\n",
    "    lr_patches = np.array(lr_patches[:num_to_keep])\n",
    "    ID_list = ID_list[:num_to_keep]\n",
    "        \n",
    "    if tr: \n",
    "        h5_file = h5py.File(out + 'train_data.hdf5', 'w')\n",
    "    else:\n",
    "        h5_file = h5py.File(out + 'validation_data.hdf5', 'w') \n",
    "        \n",
    "    # create the hdf5 file\n",
    "    data_set_lr = h5_file.create_dataset('data_lr', lr_patches.shape, np.float32)\n",
    "    data_set_lr[...] = lr_patches\n",
    "    label_set = h5_file.create_dataset('label', hr_patches.shape, np.float32)\n",
    "    label_set[...] = hr_patches\n",
    "\n",
    "    file = open(out + 'ID_list_patches_' + str(d) + '_' + str(scale), 'wb')\n",
    "    pickle.dump(ID_list, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data(how = 'training')\n",
    "create_data(how = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = h5py.File(out + 'train_data.hdf5', 'r')\n",
    "X_train = np.array(hf_train.get('data_lr')).astype('float32')\n",
    "Y_train = np.array(hf_train.get('label')).astype('float32')\n",
    "\n",
    "hf_val = h5py.File(out + 'validation_data.hdf5', 'r')\n",
    "X_val = np.array(hf_val.get('data_lr')).astype('float32')\n",
    "Y_val = np.array(hf_val.get('label')).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6656, 8192, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8192, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
