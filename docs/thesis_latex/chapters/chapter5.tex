In this thesis we faced the problem of audio super-resolution, i.e. the task of increasing the sampling rate of a given input signal by predicting its missing high-frequency content. More specifically, we aimed to estimate a complex non-linear regression function which maps a given low-resolution input signal to its corresponding high-resolution version. \\
In order to do so, we focused our attention on deep learning models, which have demonstrated state of-the-art results in many signal processing based applications. More specifically, we worked in a supervised learning setting in which we trained models on a dataset composed by low-resolution, high-resolution 8192-length recording pairs. Input sequences were obtained by downsampling the original signals by a factor of 4; for 16 k\gls{hz} high-resolution data, this corresponds to patches of approximately 500ms. \\
Data were taken from the \gls{vctk} Corpus, which is a benchmark dataset for this task; briefly, it encompasses dozens of different speakers with various accents. \\
The proposed model is based on an hybrid architecture that combines two different methods that achieved remarkable results in literature: \gls{tfnet} and \gls{tfilm} Net. \\
The former system aims to estimate the high-frequency content of an input signal through a branching architecture; authors proposed a model which utilizes both time and frequency domain, in two different branches. In other words, \gls{tfnet} is an end-to-end system which allows the two domains to be jointly optimized. In the original paper it was demonstrated the superior performance of this strategy with respect to models which operate only in either time or frequency domain. \\
On the other hand, \gls{tfilm} Net is a deep neural network which learns the low-resolution to high-resolution mapping directly in the time domain. Its main contribution lies in the integration of convolutional and recurrent approaches to efficiently incorporate long-range information. More specifically, \gls{tfilm} layers modulate the activations of a convolutional layer through a \gls{rnn}. This system resulted very effective when processing sequential data, such as audio signals. \\
The proposed model can be considered as a mix of these two methods. On one hand, the proposed system maintains the branching structure of \gls{tfnet} which allows to process audio in both the time and the frequency domain. On the other hand, our model uses \gls{tfilm} Net modules to process data in the two branches. By doing so, we inherited the advantages of the two methods, such as the branching structure and the adoption of recurrent layers which can expand the limited receptive field of \gls{cnn}s. \\
The available resources did not allow to train, validate and test all the models in their original form: therefore, convolutional layers dimension were reduced such that each model shares the same number of trainable parameters. \\
As stated in Chapter \ref{chap:exp_results}, all the training configurations, such as the learning rate, the batch size and the optimizer, were taken from the reference papers \cite{lim2018time}, \cite{birnbaum2019temporal}. \\
As for the evaluation methods, we used two standard metrics for this task, such as \gls{snr} and \gls{lsd}, and a third criteria which is less common: we investigated whether super-resolution models can help a speech-to-text engine in better performing its automated conversion task. We investigated the effect of the three models by the use of a standard evaluation metric for \gls{stt} task such as the \gls{wer}. As a further work, we mention that it would be interesting to include qualitative metrics based on human perception of audio quality. These would certainly be the natural complement of the objective evaluations that we proposed in this work. \\
Experimental results are quite encouraging: the proposed model outperformed previous super resolution approaches on the \gls{lsd} metric, which takes into account the reconstruction quality in the spectral domain. On the other hand, we have \gls{snr}, which measures the difference between the model signal reconstruction and the ground-truth data in the time domain; according to this metric \gls{tfnet} was the system which achieves the best value. As for the \gls{wer} metric, our model showed actual improvements in helping the Deep Speech engine in the \gls{stt} conversion. \\
These results suggest that the proposed method provides a remarkable reconstruction quality. However, our Spectral Fusion Layer weights analysis described in Chapter \ref{chap:exp_results} revealed a limitation: the frequency branch of the proposed model, whose only goal is to contribute to the spectral magnitude estimation, turned out to be only partially relevant in the final prediction. This result suggested that the architecture of this branch can be better developed to improve the performance, and this can be considered as one main improvement of our approach.\\
Furthermore, it is worth highlighting that computational limitations of the available resources were an important constraint for the achievement of better results. Thus, it would certainly be interesting to evaluate the effect of our approach without a prior drastic dimensionality reduction of both the dataset size and the models' number of parameters. \\