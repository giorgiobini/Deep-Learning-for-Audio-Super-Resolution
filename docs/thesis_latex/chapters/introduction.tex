In the last decades, the rapid development and the wide diffusion of digital circuits has made it possible to store, transmit and process any type of data. As a natural consequence of this technological progress, the scientific community has shown a growing interest in designing software tools capable of solving increasingly complex tasks. This is true for many areas of science that have been revolutionized by the advent of digital technologies, such as the signal processing. The lower cost of computational power and the possibility to design very sophisticated algorithms, have favoured the spread of digital in many signal processing applications in which analog circuits were once used. Indeed, it is often easier to operate precise mathematical operations on signals through computer software rather than in analog form. Moreover, the way of how signals are processed digitally has changed considerably over the years. Recently, deep learning-based approaches have demonstrated state of-the-art results in many challenging applications for both one-dimensional (such as audio) and two-dimensional signals (for instance, images). Some of the best-known techniques for \gls{dl} modeling will be examined in the context of a specific problem, such as audio super-resolution. \\
Audio \gls{sr} refers to the task of reconstructing \gls{hr} audio data from \gls{lr}, down-sampled input sequences containing only a small fraction (15-50\%) of the original samples. Several factors have determined the choice of this topic. First, this task is useful for a wide variety of applications in many sectors, ranging from telecommunications to many other domains. For example, super resolution techniques may be applied with regard to the data compression problem. This task consists of two main phases: encoding and decoding. First, an encoder object is used to compress audio signals with a downsample-based approach. In the second phase, a decoder is utilized in order to upsample data with a super resolution algorithm. This procedure is highly useful to save storage space and can be also used to reduce data transmission time in online applications. \\
Another interesting use case can be found in the use of this model for the audio restoration task. The objective of this problem consists in reconstructing high quality signals from audio files recorded with low fidelity equipment. In this sense, audio super resolution may also come in help in forensics analysis by improving clarity of recordings. In addition, companies may also consider the application of this algorithm to save on the use of expensive signal acquisition devices. More and more often, software technologies can address physical limitations of hardware components or improve their performance through effective data processing.\\
Other possible uses could be concerned with the integration of this model with other \gls{ai} applications. For example, one might think of the audio super-resolution algorithm as a useful tool that may come in help for the speech-to-text conversion task, in which another \gls{dl} model could be facilitated in the learning process through the use of more precise and informative input data. An analogous example can concern the opposite problem, i.e. text-to-speech, where a \gls{sr} system has the potential to improve the perceived quality of an artificial audio file created by another deep learning model. \\
Audio super resolution can be useful for any kind of audio signal. However, for this thesis project, it is decided to refer univocally to data concerning the speech domain, which is of particular interest for many of the previously listed applications.

\section{Problem formulation} \label{problem_formulation}
The best way to avoid introducing any potential ambiguity is to define formally the problem, both from a notational and terminological point of view. \\
Super-resolution refers to the task of reconstructing \gls{hr} data from \gls{lr} observations. If we denote with $Y$ the high-dimensional signal and with $X$ the \gls{lr} input, we could see the audio \gls{sr} model as a mapping function $f_{\theta}$, parametrized by $\theta$, with the following form:
$$
f_{\theta}: X \rightarrow Y
$$
The above definition can be extended to include any kind of signal, such as audio or images. Without losing generality, it is possible to define a signal as a physical quantity that varies with time, space, or any other independent variable \cite{proakis2006dimitris}. However, since the object of study is limited to the audio signals domain, the focus is restricted to that type of data. \\
Audio signals can be further classified into various categories: continuous (i.e., analog) versus discrete, continuous-valued versus discrete-valued. For this study we will refer specifically to digital audio signals, i.e. discrete-valued measurements of continuous-time quantities obtained by a sampling process. \\
The preceding formulation directly leads to a specific definition of the elements $x \in X$ and $y \in Y$. It is possible to define $x = (x_{1/R_1}, x_{2/R_1}, \dots, x_{R_1S/R_1})$ as a one-dimensional vector sampled at rate $R_1$, where $S$ is the duration of the signal (in seconds). It follows that number of samples is defined as $T_1 = SR_1$. The target sequence $y = (y_{1/R_2}, y_{2/R_2}, \dots, y_{R_2S/R_2})$ has a higher sampling rate $R_2 > R_1$, and that makes higher the temporal dimension too, i.e. $T_2 > T_1$, where $T_2 = SR_2$. We can define $r = \frac{R_2}{R_1}$ to denote the upsampling ratio of the two signals, which in this work is equal to $r = 4$. The main idea of this problem formulation is that $\hat{y} = f_{\theta}(x)$, where $\hat{y} = (\hat{y}_{1/R_2}, \hat{y}_{2/R_2}, \dots, \hat{y}_{R_2S/R_2})$ is the reconstruction of the \gls{hr} signal. To obtain the target estimation, a supervised learning model is trained on a dataset $\mathcal{D} = \{x_i, y_i\}_{i = 1}^n$ composed by LR, HR pairs. The model $f_{\theta}$, that in this case is a \gls{dl} algorithm, must predict the target observation $y$ from its down-sampled version $x$, or more generally predict the conditional probability distribution $p(y|x)$. \\
As for terminology, it is important to mention that the problem of audio super-resolution is studied under the name of \gls{bwe} \cite{ekstrand2002bandwidth} by the audio processing community, so these two terms are used as synonyms in the remainder of the thesis. Furthermore, since the data used refer to speech, it would also be correct to speak about \textit{speech bandwith extension} \cite{wang2018speech}. However, the adoption of a more generic terminology is preferred, also because the techniques used in this project can theoretically be extended to other categories of audio signals, such as music or environmental sounds. In general, it is fair to say that this application can be extended for all signals in which the high-frequency part is considered highly dependent on its corresponding low-frequency counterpart, as in the case of speech. \\
Another term that is often used in this thesis is the one of \textit{system}.
A system may be defined as a device that performs an operation on a signal. The concept of system includes not only physical devices, but also software realizations of operations on a signal \cite{proakis2006dimitris}. Systems can be classified differently depending on their properties. For example, linear systems perform only linear operations on the input signal. On the contrary, if these operations are nonlinear, such as the processing of a neural network, the system is said to be nonlinear.
\newpage 
\section{Related work}
The problem of audio super resolution has been widely studied in the past few decades. Existing data-driven approaches have lead to significant improvements in the context of \gls{sr} problems with a large variety of methods. Even if non-parametric models have shown good results \cite{freeman2002example}, most methods in the literature are characterized by the use of parametric models, such as \gls{dl}, Gaussian Mixture Models \cite{cheng1994statistical}, \cite{park2000narrowband}, \cite{pulakka2011speech} or Hidden Markov Models \cite{jax2003artificial}, \cite{song2009study}. \\
Another level of categorization between existing approaches is the following: domain-dependent and domain-agnostic. The former aim to restore high-frequency information from low sample rate audio by the extraction of handcrafted features and designing ad-hoc learning strategies (see e.g., \cite{pulakka2011speech}). More recently, the algorithms used to model audio allows the direct use of raw data as input. This is another advantage of \gls{dl} models: they are often effective even without a manual feature extraction. Furthermore, many of these \gls{dl} algorithms are fully domain-agnostic, i.e. their use can be extended for different type of task, such as Text Classification (see e.g. \cite{birnbaum2019temporal}). \\
Deep learning-based approaches for audio \gls{sr} have changed considerably over the last few years. Traditional \gls{dnn}-based techniques \cite{li2015dnn} have been superseded by \gls{cnn} models \cite{kuleshov2017audio}. The latest improvements are associated with the use of \gls{rnn} \cite{birnbaum2019temporal} and \gls{gan} \cite{eskimez2019speech}. The former can take into account the sequential nature of the audio data, the latter is particularly suitable for generative tasks. Although \gls{gan}s yield remarkable results in many digital signal processing problems, they suffer from instabilities during the training phase \cite{mescheder2018training}. Because of this intrinsic difficulty, this family of models is not included in the rest of the study. \\ 
It is also important to mention that a satisfactory result for this task has been obtained by the use of auto-regressive models, such as WaveNet \cite{wang2018speech}. \\
Results of recent papers indicate that there is not a clear winner between all these different methods. This is also due to the fact that, when comparing models, many aspects must be considered, such as computational time (on which a possible application in real-time depends). However, there are some standard evaluation metrics currently used in \gls{sr} problems that can be used for comparison. All the aspects related to model evaluation are explained in detail in the body of the work. 
\newpage 
\section{Research Objectives}
The study of recent literature papers has advanced the idea of combining different methods that performed well in other studies. Indeed, the key thrust of this thesis is on the implementation of a novel model architecture inspired by some of the state-of-the-art techniques. The two studies from which most of the proposed methods in this work derives are the following:
\begin{itemize}
	\item \textbf{Time-frequency networks for audio super-resolution} \cite{lim2018time}: This work introduces \gls{tfnet}, a deep neural network that consists of two branches with similar architectures; a time domain branch and a frequency domain branch, which explicitly models the reconstruction’s spectral magnitude. Each domain is modeled jointly using design patterns from AudioUNet \cite{kuleshov2017audio}, a fully convolutional model consisting of encoder and decoder blocks.
	\item \textbf{Temporal FiLM} \cite{birnbaum2019temporal}:  Birnbaum, Kuleshov \textit{et al.} propose \gls{tfilm}, an innovative neural network component that uses a \gls{rnn} to alter the activations of a convolutional model. This approach aims to capture long-range information in sequential data processing by the adoption of recurrent layers that can expand the receptive field of \gls{cnn}s.
\end{itemize}
The proposed architecture is intended to directly exploit the best of both works, in order to explore a novel model configuration. The key idea is to replace the AudioUNet components used by Lim, Yeh \textit{et al.} with the \gls{tfilm} layers cited above. The model maintains the branching structure that allows to process audio in both time and frequency domain. A detailed exposition of the most interesting features is covered in detail in Chapter 3.

\section{Thesis Organization}
This work is organised into five chapters, including the present one. A brief overview of the thesis structure is provided below. 
\begin{itemize}
	\item \textbf{Chapter 2} treats the main theoretical aspects that need to be covered in addressing the audio \gls{sr} problem by using \gls{dl} algorithms to process data. These concepts include basic elements of \gls{dsp}, \gls{ml} and \gls{dl}.
	\item \textbf{Chapter 3} is devoted entirely to the characterization and analysis of the proposed model. All the main features are highlighted in detail to provide a precise and comprehensible description. 
	\item \textbf{Chapter 4} describes the experiments performed and the results obtained. A particular focus is given to description of the dataset as well as the evaluation metrics used. 
	\item \textbf{Chapter 5} we conclude the thesis and discuss possible future directions to advance the field.
\end{itemize}
