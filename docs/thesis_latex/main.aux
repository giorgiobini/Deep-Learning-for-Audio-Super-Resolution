\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\bibstyle{biblatex}
\bibdata{main-blx,references}
\citation{biblatex-control}
\abx@aux@refcontext{ynt/global//global/global}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{main.ist}
\@glsorder{word}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{5}{section*.2}\protected@file@percent }
\gdef \LT@i {\LT@entry 
    {1}{56.69603pt}\LT@entry 
    {1}{246.00238pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{15}{chapter.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:intro}{{1}{15}{Introduction}{chapter.1}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{ekstrand2002bandwidth}
\abx@aux@cite{ekstrand2002bandwidth}
\abx@aux@segm{0}{0}{ekstrand2002bandwidth}
\citation{wang2018speech}
\abx@aux@cite{wang2018speech}
\abx@aux@segm{0}{0}{wang2018speech}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem formulation}{16}{section.1.1}\protected@file@percent }
\newlabel{problem_formulation}{{1.1}{16}{Problem formulation}{section.1.1}{}}
\citation{freeman2002example}
\abx@aux@cite{freeman2002example}
\abx@aux@segm{0}{0}{freeman2002example}
\citation{cheng1994statistical}
\abx@aux@cite{cheng1994statistical}
\abx@aux@segm{0}{0}{cheng1994statistical}
\citation{park2000narrowband}
\abx@aux@cite{park2000narrowband}
\abx@aux@segm{0}{0}{park2000narrowband}
\citation{pulakka2011speech}
\abx@aux@cite{pulakka2011speech}
\abx@aux@segm{0}{0}{pulakka2011speech}
\citation{jax2003artificial}
\abx@aux@cite{jax2003artificial}
\abx@aux@segm{0}{0}{jax2003artificial}
\citation{song2009study}
\abx@aux@cite{song2009study}
\abx@aux@segm{0}{0}{song2009study}
\citation{pulakka2011speech}
\abx@aux@cite{pulakka2011speech}
\abx@aux@segm{0}{0}{pulakka2011speech}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{li2015dnn}
\abx@aux@cite{li2015dnn}
\abx@aux@segm{0}{0}{li2015dnn}
\citation{kuleshov2017audio}
\abx@aux@cite{kuleshov2017audio}
\abx@aux@segm{0}{0}{kuleshov2017audio}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{eskimez2019speech}
\abx@aux@cite{eskimez2019speech}
\abx@aux@segm{0}{0}{eskimez2019speech}
\citation{mescheder2018training}
\abx@aux@cite{mescheder2018training}
\abx@aux@segm{0}{0}{mescheder2018training}
\citation{wang2018speech}
\abx@aux@cite{wang2018speech}
\abx@aux@segm{0}{0}{wang2018speech}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related work}{18}{section.1.2}\protected@file@percent }
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{kuleshov2017audio}
\abx@aux@cite{kuleshov2017audio}
\abx@aux@segm{0}{0}{kuleshov2017audio}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}Research Objectives}{19}{section.1.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Organization}{19}{section.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminary Concepts of Signal Processing and Deep Learning}{21}{chapter.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:theory}{{2}{21}{Preliminary Concepts of Signal Processing and Deep Learning}{chapter.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Digital Signal Processing}{21}{section.2.1}\protected@file@percent }
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Classification of Signals}{22}{subsection.2.1.1}\protected@file@percent }
\citation{avanzini2005fundamentals}
\abx@aux@cite{avanzini2005fundamentals}
\abx@aux@segm{0}{0}{avanzini2005fundamentals}
\citation{avanzini2005fundamentals}
\abx@aux@cite{avanzini2005fundamentals}
\abx@aux@segm{0}{0}{avanzini2005fundamentals}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  (a) \textit  {Analog}, (b) \textit  {quantized-analog}, (c) \textit  {sampled}, and (d) \textit  {digital} signals. From \cite {avanzini2005fundamentals}.\relax }}{23}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:signals}{{2.1}{23}{(a) \textit {Analog}, (b) \textit {quantized-analog}, (c) \textit {sampled}, and (d) \textit {digital} signals. From \cite {avanzini2005fundamentals}.\relax }{figure.caption.7}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}From Analog to Digital}{24}{subsection.2.1.2}\protected@file@percent }
\newlabel{from_a_to_d}{{2.1.2}{24}{From Analog to Digital}{subsection.2.1.2}{}}
\newlabel{eq:from_a_to_d}{{2.2}{24}{From Analog to Digital}{equation.2.1.2}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\newlabel{eq:tn_relation}{{2.3}{25}{From Analog to Digital}{equation.2.1.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Periodic sampling of an analog signal. From \cite {proakis2006dimitris}.\relax }}{25}{figure.caption.8}\protected@file@percent }
\newlabel{fig:periodic_sampling}{{2.2}{25}{Periodic sampling of an analog signal. From \cite {proakis2006dimitris}.\relax }{figure.caption.8}{}}
\newlabel{eq:sampling_theorem}{{2.4}{25}{From Analog to Digital}{equation.2.1.4}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  Illustration of aliasing; the sampling frequency $F_{s} = 1$ \gls {hz} is not sufficiently high to unambiguously reconstruct the original sinusoid. From \cite {proakis2006dimitris}.\relax }}{26}{figure.caption.9}\protected@file@percent }
\newlabel{fig:aliasing}{{2.3}{26}{Illustration of aliasing; the sampling frequency $F_{s} = 1$ \gls {hz} is not sufficiently high to unambiguously reconstruct the original sinusoid. From \cite {proakis2006dimitris}.\relax }{figure.caption.9}{}}
\newlabel{eq:quantizer}{{2.5}{26}{From Analog to Digital}{equation.2.1.5}{}}
\newlabel{eq:quant_noise}{{2.6}{26}{From Analog to Digital}{equation.2.1.6}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Graphical example of quantization. From \cite {proakis2006dimitris}.\relax }}{27}{figure.caption.10}\protected@file@percent }
\newlabel{fig:quantization}{{2.4}{27}{Graphical example of quantization. From \cite {proakis2006dimitris}.\relax }{figure.caption.10}{}}
\citation{christensen2019sound}
\abx@aux@cite{christensen2019sound}
\abx@aux@segm{0}{0}{christensen2019sound}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Numerical illustration of quantization with one significant digit using rounding. From \cite {proakis2006dimitris}.\relax }}{28}{table.caption.11}\protected@file@percent }
\newlabel{tab:quant}{{2.1}{28}{Numerical illustration of quantization with one significant digit using rounding. From \cite {proakis2006dimitris}.\relax }{table.caption.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}About Sinusoids}{28}{subsection.2.1.3}\protected@file@percent }
\newlabel{eq:sinusoidal_signal}{{2.7}{28}{About Sinusoids}{equation.2.1.7}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of a discrete-time sinusoidal signal ($\omega = \frac  {\pi }{6}$ and $\phi = \frac  {\pi }{3}$). From \cite {proakis2006dimitris}.\relax }}{29}{figure.caption.12}\protected@file@percent }
\newlabel{fig:sinusoidal_signal}{{2.5}{29}{Example of a discrete-time sinusoidal signal ($\omega = \frac {\pi }{6}$ and $\phi = \frac {\pi }{3}$). From \cite {proakis2006dimitris}.\relax }{figure.caption.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Complex sinusoids}{29}{section*.13}\protected@file@percent }
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Classification of Discrete-Time Signals}{30}{subsection.2.1.4}\protected@file@percent }
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Example of even (a) and odd (b) signals. From \cite {proakis2006dimitris}.\relax }}{31}{figure.caption.14}\protected@file@percent }
\newlabel{fig:evenodd}{{2.6}{31}{Example of even (a) and odd (b) signals. From \cite {proakis2006dimitris}.\relax }{figure.caption.14}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Spectral Analysis of Signals}{32}{subsection.2.1.5}\protected@file@percent }
\newlabel{fourier}{{2.1.5}{32}{Spectral Analysis of Signals}{subsection.2.1.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Discrete-Time Fourier Series}{32}{section*.15}\protected@file@percent }
\newlabel{Synthesis_eq_fs}{{2.15}{32}{Discrete-Time Fourier Series}{equation.2.1.15}{}}
\newlabel{harmonic}{{2.16}{32}{Discrete-Time Fourier Series}{equation.2.1.16}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\newlabel{analysis_eq_fs}{{2.17}{33}{Discrete-Time Fourier Series}{equation.2.1.17}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Discrete-Time Fourier Transform}{33}{section*.16}\protected@file@percent }
\newlabel{eq:analysis dtft}{{2.18}{33}{Discrete-Time Fourier Transform}{equation.2.1.18}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\newlabel{eq:synthesis dtft}{{2.19}{34}{Discrete-Time Fourier Transform}{equation.2.1.19}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Discrete Fourier Transform}{35}{section*.17}\protected@file@percent }
\newlabel{eq:dft}{{2.20}{35}{Discrete Fourier Transform}{equation.2.1.20}{}}
\newlabel{eq:idft}{{2.21}{35}{Discrete Fourier Transform}{equation.2.1.21}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Discrete-time Systems}{36}{subsection.2.1.6}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Block diagram representation of a discrete-time system. From \cite {proakis2006dimitris}.\relax }}{37}{figure.caption.18}\protected@file@percent }
\newlabel{fig:dtsystem}{{2.7}{37}{Block diagram representation of a discrete-time system. From \cite {proakis2006dimitris}.\relax }{figure.caption.18}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Convolution}{38}{section*.19}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Graphical representation of the unit sample signal. From \cite {proakis2006dimitris}.\relax }}{38}{figure.caption.20}\protected@file@percent }
\newlabel{fig:unit}{{2.8}{38}{Graphical representation of the unit sample signal. From \cite {proakis2006dimitris}.\relax }{figure.caption.20}{}}
\newlabel{eq:decomposition_xn}{{2.26}{38}{Convolution}{equation.2.1.26}{}}
\citation{proakis2006dimitris}
\abx@aux@cite{proakis2006dimitris}
\abx@aux@segm{0}{0}{proakis2006dimitris}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\newlabel{eq:superp_summ}{{2.27}{39}{Convolution}{equation.2.1.27}{}}
\newlabel{eq:lti_resp}{{2.28}{39}{Convolution}{equation.2.1.28}{}}
\newlabel{eq:conv_sum}{{2.30}{39}{Convolution}{equation.2.1.30}{}}
\citation{rai2019editor}
\abx@aux@cite{rai2019editor}
\abx@aux@segm{0}{0}{rai2019editor}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{mitchell1997machine}
\abx@aux@cite{mitchell1997machine}
\abx@aux@segm{0}{0}{mitchell1997machine}
\citation{hady2013semi}
\abx@aux@cite{hady2013semi}
\abx@aux@segm{0}{0}{hady2013semi}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\newlabel{conv_theorem}{{2.34}{40}{Convolution}{equation.2.1.34}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Deep Learning}{40}{section.2.2}\protected@file@percent }
\citation{ILSVRC15}
\abx@aux@cite{ILSVRC15}
\abx@aux@segm{0}{0}{ILSVRC15}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces A Venn diagram showing the conceptual location of \gls {dl}, \gls {ml} and \gls {ai} in the terms hierarchy. We can consider \gls {ai} as the term with the wider range of uses. Each section of the diagram includes an example of an \gls {ai} technology. From \cite {goodfellow2016deep}.\relax }}{42}{figure.caption.21}\protected@file@percent }
\newlabel{fig:ai}{{2.9}{42}{A Venn diagram showing the conceptual location of \gls {dl}, \gls {ml} and \gls {ai} in the terms hierarchy. We can consider \gls {ai} as the term with the wider range of uses. Each section of the diagram includes an example of an \gls {ai} technology. From \cite {goodfellow2016deep}.\relax }{figure.caption.21}{}}
\citation{kiefer1952stochastic}
\abx@aux@cite{kiefer1952stochastic}
\abx@aux@segm{0}{0}{kiefer1952stochastic}
\citation{kingma2014adam}
\abx@aux@cite{kingma2014adam}
\abx@aux@segm{0}{0}{kingma2014adam}
\citation{dozat2016incorporating}
\abx@aux@cite{dozat2016incorporating}
\abx@aux@segm{0}{0}{dozat2016incorporating}
\citation{amari1993backpropagation}
\abx@aux@cite{amari1993backpropagation}
\abx@aux@segm{0}{0}{amari1993backpropagation}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Learning Process}{43}{subsection.2.2.1}\protected@file@percent }
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{wu2015deep}
\abx@aux@cite{wu2015deep}
\abx@aux@segm{0}{0}{wu2015deep}
\citation{wu2015deep}
\abx@aux@cite{wu2015deep}
\abx@aux@segm{0}{0}{wu2015deep}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Bias and Variance trade-off. As the capacity (complexity) of the model increases (x-axis), bias (dotted) tends to decrease and variance (dashed) tends to increase. The optimal capacity occurs when both bias and variance errors are minimized. From \cite {goodfellow2016deep}.\relax }}{45}{figure.caption.22}\protected@file@percent }
\newlabel{fig:underoverfitting}{{2.10}{45}{Bias and Variance trade-off. As the capacity (complexity) of the model increases (x-axis), bias (dotted) tends to decrease and variance (dashed) tends to increase. The optimal capacity occurs when both bias and variance errors are minimized. From \cite {goodfellow2016deep}.\relax }{figure.caption.22}{}}
\citation{srivastava2014dropout}
\abx@aux@cite{srivastava2014dropout}
\abx@aux@segm{0}{0}{srivastava2014dropout}
\citation{krizhevsky2012imagenet}
\abx@aux@cite{krizhevsky2012imagenet}
\abx@aux@segm{0}{0}{krizhevsky2012imagenet}
\citation{ILSVRC15}
\abx@aux@cite{ILSVRC15}
\abx@aux@segm{0}{0}{ILSVRC15}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Examples of color augmentations tested by Wu et al \cite {wu2015deep}.\relax }}{46}{figure.caption.23}\protected@file@percent }
\newlabel{fig:data_aug}{{2.11}{46}{Examples of color augmentations tested by Wu et al \cite {wu2015deep}.\relax }{figure.caption.23}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Convolutional Neural Networks}{46}{subsection.2.2.2}\protected@file@percent }
\newlabel{cnns}{{2.2.2}{46}{Convolutional Neural Networks}{subsection.2.2.2}{}}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Graphical representation of the connections between input and output units in both convolutional and fully connected approaches. \textit  {Top:} When we use \gls {cnn} layers with a kernel of width 3, the connectivity is sparse such that each output unit is affected only by 3 input units. \textit  {Bottom:} In fully connected layers each output unit is formed by matrix multiplication, so that all of the inputs affect the highlighted $s_3$. From \cite {goodfellow2016deep}.\relax }}{47}{figure.caption.24}\protected@file@percent }
\newlabel{fig:sparse_interactions}{{2.12}{47}{Graphical representation of the connections between input and output units in both convolutional and fully connected approaches. \textit {Top:} When we use \gls {cnn} layers with a kernel of width 3, the connectivity is sparse such that each output unit is affected only by 3 input units. \textit {Bottom:} In fully connected layers each output unit is formed by matrix multiplication, so that all of the inputs affect the highlighted $s_3$. From \cite {goodfellow2016deep}.\relax }{figure.caption.24}{}}
\citation{zeiler2014visualizing}
\abx@aux@cite{zeiler2014visualizing}
\abx@aux@segm{0}{0}{zeiler2014visualizing}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Deeper layers can indirectly interact with a larger portion of the input. From \cite {goodfellow2016deep}.\relax }}{48}{figure.caption.25}\protected@file@percent }
\newlabel{fig:deeperlayers}{{2.13}{48}{Deeper layers can indirectly interact with a larger portion of the input. From \cite {goodfellow2016deep}.\relax }{figure.caption.25}{}}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces A convolutional neural network extracts increasingly abstract features from an image. From \cite {goodfellow2016deep}.\relax }}{49}{figure.caption.26}\protected@file@percent }
\newlabel{fig:cnnviz}{{2.14}{49}{A convolutional neural network extracts increasingly abstract features from an image. From \cite {goodfellow2016deep}.\relax }{figure.caption.26}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Recurrent Neural Networks}{49}{subsection.2.2.3}\protected@file@percent }
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces "The computational graph to compute the training loss of a recurrent network that maps an input sequence of \textbf  {x} values to a corresponding sequence of output \textbf  {o} values. A loss \textbf  {L} measures how far each \textbf  {o} is from the corresponding training target \textbf  {y}. When using softmax outputs, we assume \textbf  {o} is the unnormalized log probabilities. The loss \textbf  {L} internally computes \textbf  {$\hat  {y}$} = softmax(\textbf  {o}) and compares this to the target \textbf  {y}. The \gls {rnn} has input to hidden connections parametrized by a weight matrix \textbf  {U}, hidden-to-hidden recurrent connections parametrized by a weight matrix \textbf  {W}, and hidden-to-output connections parametrized by a weight matrix \textbf  {V}". \textit  {Left:} Circuit diagram representation. \textit  {Right:} The same \gls {rnn} seen as an unfolded computational graph. From \cite {goodfellow2016deep}.\relax }}{50}{figure.caption.27}\protected@file@percent }
\newlabel{fig:rnn_graph}{{2.15}{50}{"The computational graph to compute the training loss of a recurrent network that maps an input sequence of \textbf {x} values to a corresponding sequence of output \textbf {o} values. A loss \textbf {L} measures how far each \textbf {o} is from the corresponding training target \textbf {y}. When using softmax outputs, we assume \textbf {o} is the unnormalized log probabilities. The loss \textbf {L} internally computes \textbf {$\hat {y}$} = softmax(\textbf {o}) and compares this to the target \textbf {y}. The \gls {rnn} has input to hidden connections parametrized by a weight matrix \textbf {U}, hidden-to-hidden recurrent connections parametrized by a weight matrix \textbf {W}, and hidden-to-output connections parametrized by a weight matrix \textbf {V}". \textit {Left:} Circuit diagram representation. \textit {Right:} The same \gls {rnn} seen as an unfolded computational graph. From \cite {goodfellow2016deep}.\relax }{figure.caption.27}{}}
\citation{rumelhart1985learning}
\abx@aux@cite{rumelhart1985learning}
\abx@aux@segm{0}{0}{rumelhart1985learning}
\citation{werbos1990backpropagation}
\abx@aux@cite{werbos1990backpropagation}
\abx@aux@segm{0}{0}{werbos1990backpropagation}
\citation{hochreiter1997long}
\abx@aux@cite{hochreiter1997long}
\abx@aux@segm{0}{0}{hochreiter1997long}
\citation{chung2014empirical}
\abx@aux@cite{chung2014empirical}
\abx@aux@segm{0}{0}{chung2014empirical}
\citation{goodfellow2016deep}
\abx@aux@cite{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Time-unfolded \gls {rnn} with a single output at the end of the sequence. From \cite {goodfellow2016deep}.\relax }}{51}{figure.caption.28}\protected@file@percent }
\newlabel{fig:rnn_single_output}{{2.16}{51}{Time-unfolded \gls {rnn} with a single output at the end of the sequence. From \cite {goodfellow2016deep}.\relax }{figure.caption.28}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods for Artificial Bandwidth Extension}{53}{chapter.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methods}{{3}{53}{Methods for Artificial Bandwidth Extension}{chapter.3}{}}
\newlabel{eq:regression_formulation}{{3.1}{53}{Methods for Artificial Bandwidth Extension}{equation.3.0.1}{}}
\newlabel{eq:regression_loss}{{3.2}{53}{Methods for Artificial Bandwidth Extension}{equation.3.0.2}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{pathak2016context}
\abx@aux@cite{pathak2016context}
\abx@aux@segm{0}{0}{pathak2016context}
\citation{yeh2017semantic}
\abx@aux@cite{yeh2017semantic}
\abx@aux@segm{0}{0}{yeh2017semantic}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}TFNet}{54}{section.3.1}\protected@file@percent }
\newlabel{tfnet}{{3.1}{54}{TFNet}{section.3.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textit  {Top row}: Examples of image \gls {sr} and semantic image inpainting reconstruction. \textit  {Bottom row}: Illustration of the input/output for audio \gls {sr} in time and frequency domain. From \cite {lim2018time}.\relax }}{55}{figure.caption.29}\protected@file@percent }
\newlabel{fig:tfnet_intuition}{{3.1}{55}{\textit {Top row}: Examples of image \gls {sr} and semantic image inpainting reconstruction. \textit {Bottom row}: Illustration of the input/output for audio \gls {sr} in time and frequency domain. From \cite {lim2018time}.\relax }{figure.caption.29}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{kuleshov2017audio}
\abx@aux@cite{kuleshov2017audio}
\abx@aux@segm{0}{0}{kuleshov2017audio}
\citation{aytar2016soundnet}
\abx@aux@cite{aytar2016soundnet}
\abx@aux@segm{0}{0}{aytar2016soundnet}
\citation{shi2016real}
\abx@aux@cite{shi2016real}
\abx@aux@segm{0}{0}{shi2016real}
\citation{zhang2016colorful}
\abx@aux@cite{zhang2016colorful}
\abx@aux@segm{0}{0}{zhang2016colorful}
\citation{kuleshov2017audio}
\abx@aux@cite{kuleshov2017audio}
\abx@aux@segm{0}{0}{kuleshov2017audio}
\citation{kuleshov2017audio}
\abx@aux@cite{kuleshov2017audio}
\abx@aux@segm{0}{0}{kuleshov2017audio}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Overall pipeline of \gls {tfnet}. The system exploits both time and frequency domain information in order to map the \gls {lr} input $x$ to the \gls {hr} reconstruction $\hat  {y}$. From \cite {lim2018time}.\relax }}{56}{figure.caption.30}\protected@file@percent }
\newlabel{fig:tfnet_pipeline}{{3.2}{56}{Overall pipeline of \gls {tfnet}. The system exploits both time and frequency domain information in order to map the \gls {lr} input $x$ to the \gls {hr} reconstruction $\hat {y}$. From \cite {lim2018time}.\relax }{figure.caption.30}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}AudioUNet}{57}{subsection.3.1.1}\protected@file@percent }
\newlabel{audiounet}{{3.1.1}{57}{AudioUNet}{subsection.3.1.1}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces AudioUNet architecture. It consists of B successive downsampling/upsampling blocks linked by residual connections. From \cite {kuleshov2017audio}.\relax }}{58}{figure.caption.31}\protected@file@percent }
\newlabel{fig:audiounet}{{3.3}{58}{AudioUNet architecture. It consists of B successive downsampling/upsampling blocks linked by residual connections. From \cite {kuleshov2017audio}.\relax }{figure.caption.31}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Spectral Replicator}{58}{subsection.3.1.2}\protected@file@percent }
\newlabel{spectral_replicator}{{3.1.2}{58}{Spectral Replicator}{subsection.3.1.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Spectral Fusion Layer}{59}{subsection.3.1.3}\protected@file@percent }
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Spectrogram showing how the signal is processed through the Spectral Replicator layer. The low-frequency components of the input spectrum are replicated three times in order to replace zeros in the high-frequency counterpart. From \cite {lim2018time}.\relax }}{60}{figure.caption.32}\protected@file@percent }
\newlabel{fig:spectral_replicator}{{3.4}{60}{Spectrogram showing how the signal is processed through the Spectral Replicator layer. The low-frequency components of the input spectrum are replicated three times in order to replace zeros in the high-frequency counterpart. From \cite {lim2018time}.\relax }{figure.caption.32}{}}
\newlabel{eq:spectralfusion1}{{3.3}{60}{Spectral Fusion Layer}{equation.3.1.3}{}}
\newlabel{eq:spectralfusion2}{{3.4}{60}{Spectral Fusion Layer}{equation.3.1.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}TFiLM}{60}{section.3.2}\protected@file@percent }
\newlabel{tfilm}{{3.2}{60}{TFiLM}{section.3.2}{}}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The \gls {tfilm} layer in detail. In this example the 1D tensor of convolutional activations has the following shape: $F \in \mathbf  {R}^{8 \times 2}$. The 2 blocks are first processed by a Max Pooling layer (whit a pooling factor of 2) and then by the \gls {rnn}. Finally, the output of the recurrent network is used to modulate the convolutional layer.From \cite {birnbaum2019temporal}.\relax }}{62}{figure.caption.33}\protected@file@percent }
\newlabel{fig:tflim_layer}{{3.5}{62}{The \gls {tfilm} layer in detail. In this example the 1D tensor of convolutional activations has the following shape: $F \in \mathbf {R}^{8 \times 2}$. The 2 blocks are first processed by a Max Pooling layer (whit a pooling factor of 2) and then by the \gls {rnn}. Finally, the output of the recurrent network is used to modulate the convolutional layer.From \cite {birnbaum2019temporal}.\relax }{figure.caption.33}{}}
\citation{kuleshov2017audio}
\abx@aux@cite{kuleshov2017audio}
\abx@aux@segm{0}{0}{kuleshov2017audio}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \textit  {Top}: \gls {tfilm} Net architecture used for audio super-resolution. It consists of K downsampling blocks followed by a bottleneck layer and K upsampling blocks; features are reused via symmetric residual skip connections. \textit  {Bottom}: Internal structure of downsampling and upsampling convolutional blocks. From \cite {birnbaum2019temporal}.\relax }}{63}{figure.caption.34}\protected@file@percent }
\newlabel{fig:tflim_architecture}{{3.6}{63}{\textit {Top}: \gls {tfilm} Net architecture used for audio super-resolution. It consists of K downsampling blocks followed by a bottleneck layer and K upsampling blocks; features are reused via symmetric residual skip connections. \textit {Bottom}: Internal structure of downsampling and upsampling convolutional blocks. From \cite {birnbaum2019temporal}.\relax }{figure.caption.34}{}}
\citation{yu2015multi}
\abx@aux@cite{yu2015multi}
\abx@aux@segm{0}{0}{yu2015multi}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Overall pipeline of \gls {tfilm} Net. The system integrates convolutional and recurrent layers to efficiently incorporate long-term input dependencies by operating in the time-domain.\relax }}{64}{figure.caption.35}\protected@file@percent }
\newlabel{fig:tfilm_pipeline}{{3.7}{64}{Overall pipeline of \gls {tfilm} Net. The system integrates convolutional and recurrent layers to efficiently incorporate long-term input dependencies by operating in the time-domain.\relax }{figure.caption.35}{}}
\citation{yu2015multi}
\abx@aux@cite{yu2015multi}
\abx@aux@segm{0}{0}{yu2015multi}
\citation{yu2015multi}
\abx@aux@cite{yu2015multi}
\abx@aux@segm{0}{0}{yu2015multi}
\citation{dumoulin2016learned}
\abx@aux@cite{dumoulin2016learned}
\abx@aux@segm{0}{0}{dumoulin2016learned}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Dilated Convolution}{65}{subsection.3.2.1}\protected@file@percent }
\newlabel{dilated_conv}{{3.2.1}{65}{Dilated Convolution}{subsection.3.2.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces (a) 1-Dilated Convolution has a receptive field of $3 \times 3$. (b) 2-Dilated Convolution has a receptive field of $7 \times 7$. (c) 4-Dilated Convolution has a receptive field of $15 \times 15$. The number of parameters associated with each layer is identical. The receptive field grows exponentially while the number of parameters grows linearly. From \cite {yu2015multi}.\relax }}{65}{figure.caption.36}\protected@file@percent }
\newlabel{fig:dilated_conv}{{3.8}{65}{(a) 1-Dilated Convolution has a receptive field of $3 \times 3$. (b) 2-Dilated Convolution has a receptive field of $7 \times 7$. (c) 4-Dilated Convolution has a receptive field of $15 \times 15$. The number of parameters associated with each layer is identical. The receptive field grows exponentially while the number of parameters grows linearly. From \cite {yu2015multi}.\relax }{figure.caption.36}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Feature-Wise Linear Modulation}{65}{subsection.3.2.2}\protected@file@percent }
\newlabel{film}{{3.2.2}{65}{Feature-Wise Linear Modulation}{subsection.3.2.2}{}}
\citation{perez2018film}
\abx@aux@cite{perez2018film}
\abx@aux@segm{0}{0}{perez2018film}
\citation{dumoulin2016learned}
\abx@aux@cite{dumoulin2016learned}
\abx@aux@segm{0}{0}{dumoulin2016learned}
\citation{kim2017dynamic}
\abx@aux@cite{kim2017dynamic}
\abx@aux@segm{0}{0}{kim2017dynamic}
\citation{hu2018squeeze}
\abx@aux@cite{hu2018squeeze}
\abx@aux@segm{0}{0}{hu2018squeeze}
\newlabel{eq:batch_norm}{{3.5}{66}{Feature-Wise Linear Modulation}{equation.3.2.5}{}}
\newlabel{eq:batch_norm2}{{3.6}{66}{Feature-Wise Linear Modulation}{equation.3.2.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Proposed Model Architecture}{66}{section.3.3}\protected@file@percent }
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Overall pipeline of the proposed model. On one hand, the system maintains the branching structure of \gls {tfnet} that allows to processes audio in both time and frequency domain. On the other hand, the model uses \gls {tfilm} Net to predict both the audio reconstruction and the spectral magnitude.\relax }}{67}{figure.caption.37}\protected@file@percent }
\newlabel{fig:proposed_pipeline}{{3.9}{67}{Overall pipeline of the proposed model. On one hand, the system maintains the branching structure of \gls {tfnet} that allows to processes audio in both time and frequency domain. On the other hand, the model uses \gls {tfilm} Net to predict both the audio reconstruction and the spectral magnitude.\relax }{figure.caption.37}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}Implementation details}{68}{section.3.4}\protected@file@percent }
\newlabel{implementation_details}{{3.4}{68}{Implementation details}{section.3.4}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experimental Results}{69}{chapter.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:exp_results}{{4}{69}{Experimental Results}{chapter.4}{}}
\newlabel{fig:colab_logo}{{\caption@xref {fig:colab_logo}{ on input line 11}}{69}{Experimental Results}{figure.caption.38}{}}
\newlabel{sub@fig:colab_logo}{{}{69}{Experimental Results}{figure.caption.38}{}}
\newlabel{fig:tensorflow_logo}{{\caption@xref {fig:tensorflow_logo}{ on input line 16}}{69}{Experimental Results}{figure.caption.38}{}}
\newlabel{sub@fig:tensorflow_logo}{{}{69}{Experimental Results}{figure.caption.38}{}}
\newlabel{fig:librosa_logo}{{\caption@xref {fig:librosa_logo}{ on input line 21}}{69}{Experimental Results}{figure.caption.38}{}}
\newlabel{sub@fig:librosa_logo}{{}{69}{Experimental Results}{figure.caption.38}{}}
\newlabel{fig:scipy_logo}{{\caption@xref {fig:scipy_logo}{ on input line 26}}{69}{Experimental Results}{figure.caption.38}{}}
\newlabel{sub@fig:scipy_logo}{{}{69}{Experimental Results}{figure.caption.38}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces From the left, the logos of Colab, Tensorflow, Librosa and Scipy.\relax }}{69}{figure.caption.38}\protected@file@percent }
\newlabel{fig:fig}{{4.1}{69}{From the left, the logos of Colab, Tensorflow, Librosa and Scipy.\relax }{figure.caption.38}{}}
\citation{yamagishi2019cstr}
\abx@aux@cite{yamagishi2019cstr}
\abx@aux@segm{0}{0}{yamagishi2019cstr}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{eskimez2019speech}
\abx@aux@cite{eskimez2019speech}
\abx@aux@segm{0}{0}{eskimez2019speech}
\citation{wang2018speech}
\abx@aux@cite{wang2018speech}
\abx@aux@segm{0}{0}{wang2018speech}
\citation{steidl2009automatic}
\abx@aux@cite{steidl2009automatic}
\abx@aux@segm{0}{0}{steidl2009automatic}
\citation{kamath2019automatic}
\abx@aux@cite{kamath2019automatic}
\abx@aux@segm{0}{0}{kamath2019automatic}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Setup}{70}{section.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Dataset and preparation}{70}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Bar chart showing the count of speakers in \gls {vctk} dataset based on their accent.\relax }}{71}{figure.caption.39}\protected@file@percent }
\newlabel{fig:accents_speakers}{{4.2}{71}{Bar chart showing the count of speakers in \gls {vctk} dataset based on their accent.\relax }{figure.caption.39}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{han2013comparison}
\abx@aux@cite{han2013comparison}
\abx@aux@segm{0}{0}{han2013comparison}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Violin plot showing the age distribution of speakers in \gls {vctk} dataset based on level of gender.\relax }}{72}{figure.caption.40}\protected@file@percent }
\newlabel{fig:age_dataset}{{4.3}{72}{Violin plot showing the age distribution of speakers in \gls {vctk} dataset based on level of gender.\relax }{figure.caption.40}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{kingma2014adam}
\abx@aux@cite{kingma2014adam}
\abx@aux@segm{0}{0}{kingma2014adam}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Data partition details for both \textit  {Single-Speaker} and \textit  {Multi-Speaker} tasks.\relax }}{73}{table.caption.41}\protected@file@percent }
\newlabel{tab:data_partition}{{4.1}{73}{Data partition details for both \textit {Single-Speaker} and \textit {Multi-Speaker} tasks.\relax }{table.caption.41}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Training Details}{73}{subsection.4.1.2}\protected@file@percent }
\newlabel{training_details}{{4.1.2}{73}{Training Details}{subsection.4.1.2}{}}
\newlabel{fig:tfnet_loss}{{\caption@xref {fig:tfnet_loss}{ on input line 93}}{74}{Training Details}{figure.caption.42}{}}
\newlabel{sub@fig:tfnet_loss}{{}{74}{Training Details}{figure.caption.42}{}}
\newlabel{fig:tfnet_snr}{{\caption@xref {fig:tfnet_snr}{ on input line 98}}{74}{Training Details}{figure.caption.42}{}}
\newlabel{sub@fig:tfnet_snr}{{}{74}{Training Details}{figure.caption.42}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \gls {tfnet} training curves on both training and validation sets. The model is trained for 127 epochs.\relax }}{74}{figure.caption.42}\protected@file@percent }
\newlabel{fig:tfnet_training_curves}{{4.4}{74}{\gls {tfnet} training curves on both training and validation sets. The model is trained for 127 epochs.\relax }{figure.caption.42}{}}
\newlabel{fig:tfilmnet_loss}{{\caption@xref {fig:tfilmnet_loss}{ on input line 108}}{74}{Training Details}{figure.caption.43}{}}
\newlabel{sub@fig:tfilmnet_loss}{{}{74}{Training Details}{figure.caption.43}{}}
\newlabel{fig:tfilmnet_snr}{{\caption@xref {fig:tfilmnet_snr}{ on input line 113}}{74}{Training Details}{figure.caption.43}{}}
\newlabel{sub@fig:tfilmnet_snr}{{}{74}{Training Details}{figure.caption.43}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces \gls {tfilm} Net training curves on both training and validation sets. The model is trained for 351 epochs.\relax }}{74}{figure.caption.43}\protected@file@percent }
\newlabel{fig:tfilmnet_training_curves}{{4.5}{74}{\gls {tfilm} Net training curves on both training and validation sets. The model is trained for 351 epochs.\relax }{figure.caption.43}{}}
\citation{gray1976distance}
\abx@aux@cite{gray1976distance}
\abx@aux@segm{0}{0}{gray1976distance}
\newlabel{fig:gionet_loss}{{\caption@xref {fig:gionet_loss}{ on input line 123}}{75}{Training Details}{figure.caption.44}{}}
\newlabel{sub@fig:gionet_loss}{{}{75}{Training Details}{figure.caption.44}{}}
\newlabel{fig:gionet_snr}{{\caption@xref {fig:gionet_snr}{ on input line 128}}{75}{Training Details}{figure.caption.44}{}}
\newlabel{sub@fig:gionet_snr}{{}{75}{Training Details}{figure.caption.44}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Proposed net training curves on both training and validation sets. The model is trained for 175 epochs.\relax }}{75}{figure.caption.44}\protected@file@percent }
\newlabel{fig:proposed_training_curves}{{4.6}{75}{Proposed net training curves on both training and validation sets. The model is trained for 175 epochs.\relax }{figure.caption.44}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Evaluation Methods}{75}{subsection.4.1.3}\protected@file@percent }
\newlabel{eval_methods}{{4.1.3}{75}{Evaluation Methods}{subsection.4.1.3}{}}
\citation{kuleshov2017audio}
\abx@aux@cite{kuleshov2017audio}
\abx@aux@segm{0}{0}{kuleshov2017audio}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\citation{hannun2014deep}
\abx@aux@cite{hannun2014deep}
\abx@aux@segm{0}{0}{hannun2014deep}
\citation{morris2004and}
\abx@aux@cite{morris2004and}
\abx@aux@segm{0}{0}{morris2004and}
\citation{morris2004and}
\abx@aux@cite{morris2004and}
\abx@aux@segm{0}{0}{morris2004and}
\citation{morris2004and}
\abx@aux@cite{morris2004and}
\abx@aux@segm{0}{0}{morris2004and}
\newlabel{eq:snr}{{4.1}{76}{Evaluation Methods}{equation.4.1.1}{}}
\newlabel{eq:lsd}{{4.2}{76}{Evaluation Methods}{equation.4.1.2}{}}
\newlabel{eq:wer}{{4.3}{76}{Evaluation Methods}{equation.4.1.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Example of $H, S, D, I$ classification on the input words my computers deaf inhe?. From \cite {morris2004and}.\relax }}{77}{figure.caption.45}\protected@file@percent }
\newlabel{fig:wer_letters}{{4.7}{77}{Example of $H, S, D, I$ classification on the input words my computers deaf inhe?. From \cite {morris2004and}.\relax }{figure.caption.45}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Results}{77}{section.4.2}\protected@file@percent }
\newlabel{results}{{4.2}{77}{Results}{section.4.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Evaluation of \gls {bwe} methods (in \gls {db}) on the Single-Speaker task over the training set in terms of \gls {snr} and \gls {lsd}. A higher \gls {snr} is better and a lower \gls {lsd} is better.\relax }}{78}{table.caption.46}\protected@file@percent }
\newlabel{tab:s_speaker_tr}{{4.2}{78}{Evaluation of \gls {bwe} methods (in \gls {db}) on the Single-Speaker task over the training set in terms of \gls {snr} and \gls {lsd}. A higher \gls {snr} is better and a lower \gls {lsd} is better.\relax }{table.caption.46}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Evaluation of \gls {bwe} methods (in \gls {db}) on the Multi-Speaker task over the training set in terms of \gls {snr} and \gls {lsd}. A higher \gls {snr} is better and a lower \gls {lsd} is better.\relax }}{78}{table.caption.47}\protected@file@percent }
\newlabel{tab:m_speaker_tr}{{4.3}{78}{Evaluation of \gls {bwe} methods (in \gls {db}) on the Multi-Speaker task over the training set in terms of \gls {snr} and \gls {lsd}. A higher \gls {snr} is better and a lower \gls {lsd} is better.\relax }{table.caption.47}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Evaluation of \gls {bwe} methods (in \gls {db}) on the Multi-Speaker task over the validation set in terms of \gls {snr} and \gls {lsd}. A higher \gls {snr} is better and a lower \gls {lsd} is better.\relax }}{78}{table.caption.48}\protected@file@percent }
\newlabel{tab:m_speaker_val}{{4.4}{78}{Evaluation of \gls {bwe} methods (in \gls {db}) on the Multi-Speaker task over the validation set in terms of \gls {snr} and \gls {lsd}. A higher \gls {snr} is better and a lower \gls {lsd} is better.\relax }{table.caption.48}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces SNR results (in \gls {db}) on Test Set for both Single-Speaker and Multi-Speaker tasks. Higher is better.\relax }}{79}{figure.caption.49}\protected@file@percent }
\newlabel{fig:snr}{{4.8}{79}{SNR results (in \gls {db}) on Test Set for both Single-Speaker and Multi-Speaker tasks. Higher is better.\relax }{figure.caption.49}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces LSD results (in \gls {db}) on Test Set for both Single-Speaker and Multi-Speaker tasks. Lower is better.\relax }}{80}{figure.caption.50}\protected@file@percent }
\newlabel{fig:lsd}{{4.9}{80}{LSD results (in \gls {db}) on Test Set for both Single-Speaker and Multi-Speaker tasks. Lower is better.\relax }{figure.caption.50}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces \gls {wer} results (in percentage) on Test Set for both Single-Speaker and Multi-Speaker tasks. \gls {wer} on original recordings is, respectively, equal to 0.03 and 0.08. Lower is better.\relax }}{80}{figure.caption.51}\protected@file@percent }
\newlabel{fig:wer}{{4.10}{80}{\gls {wer} results (in percentage) on Test Set for both Single-Speaker and Multi-Speaker tasks. \gls {wer} on original recordings is, respectively, equal to 0.03 and 0.08. Lower is better.\relax }{figure.caption.51}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Spectrogram showing how the frequency content of a 16kHz signal changes over time.\relax }}{81}{figure.caption.52}\protected@file@percent }
\newlabel{fig:original_signal_spec}{{4.11}{81}{Spectrogram showing how the frequency content of a 16kHz signal changes over time.\relax }{figure.caption.52}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Spectral Fusion Layer Weights Analysis}{81}{section*.55}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Spectrogram showing the spline reconstruction of the signal.\relax }}{82}{figure.caption.53}\protected@file@percent }
\newlabel{fig:lowres_signal_spec}{{4.12}{82}{Spectrogram showing the spline reconstruction of the signal.\relax }{figure.caption.53}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Spectrogram showing the model reconstruction of the signal.\relax }}{82}{figure.caption.54}\protected@file@percent }
\newlabel{fig:model_signal_spec}{{4.13}{82}{Spectrogram showing the model reconstruction of the signal.\relax }{figure.caption.54}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Spectral Fusion Layer weights distribution for both the \gls {tfnet} model and the proposed system.\relax }}{83}{figure.caption.56}\protected@file@percent }
\newlabel{fig:weights_results}{{4.14}{83}{Spectral Fusion Layer weights distribution for both the \gls {tfnet} model and the proposed system.\relax }{figure.caption.56}{}}
\citation{lim2018time}
\abx@aux@cite{lim2018time}
\abx@aux@segm{0}{0}{lim2018time}
\citation{birnbaum2019temporal}
\abx@aux@cite{birnbaum2019temporal}
\abx@aux@segm{0}{0}{birnbaum2019temporal}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions and Further Work}{85}{chapter.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusions}{{5}{85}{Conclusions and Further Work}{chapter.5}{}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{kiefer1952stochastic}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{gray1976distance}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{rumelhart1985learning}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{werbos1990backpropagation}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{amari1993backpropagation}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{cheng1994statistical}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1997long}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{mitchell1997machine}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{park2000narrowband}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{ekstrand2002bandwidth}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{freeman2002example}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{jax2003artificial}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{morris2004and}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{avanzini2005fundamentals}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{proakis2006dimitris}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{song2009study}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{steidl2009automatic}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{pulakka2011speech}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky2012imagenet}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{hady2013semi}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{han2013comparison}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{chung2014empirical}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{hannun2014deep}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{kingma2014adam}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{srivastava2014dropout}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{zeiler2014visualizing}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{dozat2016incorporating}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{li2015dnn}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{ILSVRC15}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{wu2015deep}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{yu2015multi}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{aytar2016soundnet}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{dumoulin2016learned}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{goodfellow2016deep}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{pathak2016context}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{shi2016real}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{zhang2016colorful}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{kim2017dynamic}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{kuleshov2017audio}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{yeh2017semantic}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{hu2018squeeze}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{lim2018time}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{mescheder2018training}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{perez2018film}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{wang2018speech}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{birnbaum2019temporal}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{christensen2019sound}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{eskimez2019speech}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{kamath2019automatic}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{rai2019editor}{ynt/global//global/global}
\abx@aux@defaultrefcontext{0}{yamagishi2019cstr}{ynt/global//global/global}
\abx@aux@defaultlabelprefix{0}{kiefer1952stochastic}{}
\abx@aux@defaultlabelprefix{0}{gray1976distance}{}
\abx@aux@defaultlabelprefix{0}{rumelhart1985learning}{}
\abx@aux@defaultlabelprefix{0}{werbos1990backpropagation}{}
\abx@aux@defaultlabelprefix{0}{amari1993backpropagation}{}
\abx@aux@defaultlabelprefix{0}{cheng1994statistical}{}
\abx@aux@defaultlabelprefix{0}{hochreiter1997long}{}
\abx@aux@defaultlabelprefix{0}{mitchell1997machine}{}
\abx@aux@defaultlabelprefix{0}{park2000narrowband}{}
\abx@aux@defaultlabelprefix{0}{ekstrand2002bandwidth}{}
\abx@aux@defaultlabelprefix{0}{freeman2002example}{}
\abx@aux@defaultlabelprefix{0}{jax2003artificial}{}
\abx@aux@defaultlabelprefix{0}{morris2004and}{}
\abx@aux@defaultlabelprefix{0}{avanzini2005fundamentals}{}
\abx@aux@defaultlabelprefix{0}{proakis2006dimitris}{}
\abx@aux@defaultlabelprefix{0}{song2009study}{}
\abx@aux@defaultlabelprefix{0}{steidl2009automatic}{}
\abx@aux@defaultlabelprefix{0}{pulakka2011speech}{}
\abx@aux@defaultlabelprefix{0}{krizhevsky2012imagenet}{}
\abx@aux@defaultlabelprefix{0}{hady2013semi}{}
\abx@aux@defaultlabelprefix{0}{han2013comparison}{}
\abx@aux@defaultlabelprefix{0}{chung2014empirical}{}
\abx@aux@defaultlabelprefix{0}{hannun2014deep}{}
\abx@aux@defaultlabelprefix{0}{kingma2014adam}{}
\abx@aux@defaultlabelprefix{0}{srivastava2014dropout}{}
\abx@aux@defaultlabelprefix{0}{zeiler2014visualizing}{}
\abx@aux@defaultlabelprefix{0}{dozat2016incorporating}{}
\abx@aux@defaultlabelprefix{0}{li2015dnn}{}
\abx@aux@defaultlabelprefix{0}{ILSVRC15}{}
\abx@aux@defaultlabelprefix{0}{wu2015deep}{}
\abx@aux@defaultlabelprefix{0}{yu2015multi}{}
\abx@aux@defaultlabelprefix{0}{aytar2016soundnet}{}
\abx@aux@defaultlabelprefix{0}{dumoulin2016learned}{}
\abx@aux@defaultlabelprefix{0}{goodfellow2016deep}{}
\abx@aux@defaultlabelprefix{0}{pathak2016context}{}
\abx@aux@defaultlabelprefix{0}{shi2016real}{}
\abx@aux@defaultlabelprefix{0}{zhang2016colorful}{}
\abx@aux@defaultlabelprefix{0}{kim2017dynamic}{}
\abx@aux@defaultlabelprefix{0}{kuleshov2017audio}{}
\abx@aux@defaultlabelprefix{0}{yeh2017semantic}{}
\abx@aux@defaultlabelprefix{0}{hu2018squeeze}{}
\abx@aux@defaultlabelprefix{0}{lim2018time}{}
\abx@aux@defaultlabelprefix{0}{mescheder2018training}{}
\abx@aux@defaultlabelprefix{0}{perez2018film}{}
\abx@aux@defaultlabelprefix{0}{wang2018speech}{}
\abx@aux@defaultlabelprefix{0}{birnbaum2019temporal}{}
\abx@aux@defaultlabelprefix{0}{christensen2019sound}{}
\abx@aux@defaultlabelprefix{0}{eskimez2019speech}{}
\abx@aux@defaultlabelprefix{0}{kamath2019automatic}{}
\abx@aux@defaultlabelprefix{0}{rai2019editor}{}
\abx@aux@defaultlabelprefix{0}{yamagishi2019cstr}{}
\gdef \@abspage@last{93}
